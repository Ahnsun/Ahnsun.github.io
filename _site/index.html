<!doctype html>
<html lang="en" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin SEO -->





<title>En Yu (‰∫éÊÅ©) - Homepage - Homepage</title>







<meta property="og:locale" content="en">
<meta property="og:site_name" content="En Yu (‰∫éÊÅ©) - Homepage">
<meta property="og:title" content="En Yu (‰∫éÊÅ©) - Homepage">


  <link rel="canonical" href="https://github.com/pages/RayeRen/acad-homepage.github.io/">
  <meta property="og:url" content="https://github.com/pages/RayeRen/acad-homepage.github.io/">



  <meta property="og:description" content="PhD student at HUST. Focusing on Multimodal Foundation Model.">









<!-- end SEO -->


<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="assets/css/main.css">

<meta http-equiv="cleartype" content="on">
<head>
  <base target="_blank">
</head>
    <link rel="apple-touch-icon" sizes="180x180" href="images/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="images/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="images/favicon-16x16.png">
<link rel="manifest" href="images/site.webmanifest">

<meta name="msapplication-TileColor" content="#000000">
<meta name="msapplication-TileImage" content="images/mstile-144x144.png?v=M44lzPylqQ">
<meta name="msapplication-config" content="images/browserconfig.xml?v=M44lzPylqQ">
<meta name="theme-color" content="#ffffff">
<link rel="stylesheet" href="assets/css/academicons.css"/>

<script type="text/x-mathjax-config"> MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber: "all" } } }); </script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>
<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/latest.js?config=TeX-MML-AM_CHTML' async></script>

<!-- end custom head snippets -->

  </head>

  <body>

    <!--[if lt IE 9]>
<div class="notice--danger align-center" style="margin: 0;">You are using an <strong>outdated</strong> browser. Please <a href="http://browsehappy.com/">upgrade your browser</a> to improve your experience.</div>
<![endif]-->
    <div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        <button><div class="navicon"></div></button>
        <ul class="visible-links">
          <li class="masthead__menu-item masthead__menu-item--lg masthead__menu-home-item"><a href="#about-me">Homepage</a></li>
          
            <li class="masthead__menu-item"><a href="/#about-me">About Me</a></li>
          
            <li class="masthead__menu-item"><a href="/#-news">News</a></li>
          
            <li class="masthead__menu-item"><a href="/#-publications">Publications</a></li>
          
            <li class="masthead__menu-item"><a href="/#-honors-and-awards">Honors and Awards</a></li>
          
            <li class="masthead__menu-item"><a href="/#-educations">Educations</a></li>
          
            <li class="masthead__menu-item"><a href="/#-internships">Internships</a></li>

            <li class="masthead__menu-item"><a href="/#-cv" onclick="window.location.href='assets/enyu_cv.pdf'; return false;">CV</a>
            </a></li>
          
        </ul>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>

    <div id="main" role="main">
      
  <div class="sidebar sticky">
  

<div itemscope itemtype="http://schema.org/Person" class="profile_box">

  <div class="author__avatar">
    <img src="images/Yu.png" class="author__avatar" alt="En Yu (‰∫éÊÅ©)">
  </div>

  <div class="author__content">
    <h3 class="author__name">En Yu (‰∫éÊÅ©)</h3>
    <p class="author__bio">Huazhong University of Science and Technology (HUST)</p>
  </div>

  <div class="author__urls-wrapper">
    <!-- <button class="btn btn--inverse">More Info & Contact</button> -->
    <ul class="author__urls social-icons">
      
        <li><div style="white-space: normal; margin-bottom: 1em;">PhD student at HUST & visiting PhD at UCSB. Focusing on Multimodal Foundation Model.</div></li>
      
      
        <li><i class="fa fa-fw fa-map-marker" aria-hidden="true"></i> Beijing, China;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;California, USA</li>
      
      
      
        <li><a href="PhD student in HUST. Focusing on Video Understanding and Generation, MLLM."><i class="fas fa-fw fa-link" aria-hidden="true"></i> Website</a></li>
      
      
        <li><a href="mailto:yuen_daniel@outlook.com"><i class="fas fa-fw fa-envelope" aria-hidden="true"></i> Email</a></li>
      
      
       
      
      
      
      
      
        <li><a href="https://dblp.org/pid/213/4929.html"><i class="ai ai-dblp ai-fw" aria-hidden="true"></i> DBLP</a></li>
      
      
      
      
      
      
        <li><a href="https://github.com/Ahnsun"><i class="fab fa-fw fa-github" aria-hidden="true"></i> Github</a></li>
      
      
      
      
      
      
      
      
      
      
      
      
      
      
        <li><a href="https://scholar.google.com/citations?user=YOUR_GOOGLE_SCHOLAR_ID"><i class="fas fa-fw fa-graduation-cap"></i> Google Scholar</a></li>
      
      
      
        <li><a href="https://orcid.org/0000-0001-6292-6384"><i class="ai ai-orcid-square ai-fw"></i> ORCID</a></li>
      
      
      
    </ul>
      <div class="author__urls_sm">
      
        <a href="PhD student at HUST & visiting PhD at UCSB. Focusing on Multimodal Foundation Model."><i class="fas fa-fw fa-link" aria-hidden="true"></i></a>
      
      
        <a href="mailto:yuen@hust.edu.cn"><i class="fas fa-fw fa-envelope" aria-hidden="true"></i></a>
      
      
       
      
      
      
      
      
        <a href="https://dblp.org/pid/213/4929.html"><i class="ai ai-dblp ai-fw" aria-hidden="true"></i></a>
      
      
      
      
      
      
        <a href="https://github.com/Ahnsun"><i class="fab fa-fw fa-github" aria-hidden="true"></i></a>
      
      
      
      
      
      
      
      
      
      
      
      
      
      
        <a href="https://scholar.google.com/citations?user=YOUR_GOOGLE_SCHOLAR_ID"><i class="fas fa-fw fa-graduation-cap"></i></a>
      
      
      
        <a href="https://orcid.org/0000-0001-6292-6384"><i class="ai ai-orcid-square ai-fw"></i></a>
      
      
      
    </div>
  </div>
</div>

  
  </div>

    
      <article class="page" itemscope itemtype="http://schema.org/CreativeWork">
        <meta itemprop="headline" content="">
        <div class="page__inner-wrap">
          <section class="page__content" itemprop="text">
            
<p><span class="anchor" id="about-me"></span></p>

<p>Hi, I am <strong>En Yu</strong> (‰∫éÊÅ© in Chinese), a PhD student at <a href="https://www.hust.edu.cn/">Huazhong University of Science and Technology <strong><span id="total_cit">(HUST)</span></strong></a> and visting PhD at <a href="https://www.hust.edu.cn/">University of California, Santa Barbara <strong><span id="total_cit">(UCSB)</span></strong></a>, cooperated with Prof.<a href="https://sites.cs.ucsb.edu/~william/">William Wang</a>.  I am currently interning at the Foundation Model Group of  <a href="https://www.stepfun.com/chats/new">StepFun AI</a>, where I work with Prof. <a href="https://scholar.google.com.hk/citations?user=yuB-cfoAAAAJ&amp;hl=en">Xiangyu Zhang</a> and Dr. <a href="https://joker316701882.github.io/">Zheng Ge</a>.</p>

<p>My research interest includes (1) Perception, Understanding and Reasoning with Multimodal LLMs, and (2) Spatial Intelligence of Visual and Multimodal Foundation Models. I have published several papers <a href="https://scholar.google.com/citations?user=rWCQMNgAAAAJ&amp;hl=en"></a> at the top-level international AI conferences including ICLR, CVPR, ECCV, AAAI, ICML, etc. My next goal is to further build powerful multimodal foundation models and develop multimodal agents based on the foundation model to deal with complex real-world tasks, e.g., navigation and UI-assistant.</p>

<p>üé∫üé∫ <strong>I am set to graduate with my Ph.D. in June 2026 and am currently on the lookout for postdoctoral positions. If you are interested, please feel free to reach out to me via email !</strong></p>

<h1 id="-news">üî• News</h1>
<ul>
  <li><em>2025.04</em>: ¬†üéâüéâ We present <a href="https://arxiv.org/pdf/2504.07954">Perception-R1</a>. This work takes a pioneering step in exploring the potential of rule-based RL in MLLM post-training for perception policy learning.</li>

  <li><em>2025.02</em>: ¬†üéâüéâ Glad to announce that we have two papers, <a href="https://ahnsun.github.io/UTR/">Video-UTR</a> and <a href="https://arxiv.org/pdf/2503.10616">OVTR</a>, accepted for poster presentations at ICLR 2025! Let's see and have a chat in Singapore!</li>

  <li><em>2024.11</em>: ¬†üéâüéâ We present <a href="https://arxiv.org/pdf/2503.10616">OVTR</a>, the first fully end-to-end open-vocabulary multiple objects tracking framework.</li>

  <li><em>2024.11</em>: ¬†üéâüéâ We present <a href="https://ahnsun.github.io/UTR/">Video-UTR</a>, investigating the shortcut learning in video multimodal large language models and systemally establish temporal hacking theory.</li>

  <li><em>2024.07</em>: ¬†üéâüéâ Really excited to head to UCSB for a year-long PhD visiting in Prof. William Wang's NLP lab. Looking forward to boosting my research ability. Catch you all in California!</li>

  <li><em>2024.06</em>: ¬†üéâüéâ Glad to announce that our work, <a href="https://eccv.ecva.net/virtual/2024/poster/1827">Merlin</a>, has been accepted for a poster presentation at ECCV 2024! See you in Milan!</li>

  <li><em>2024.02</em>: ¬†üéâüéâ Glad to announce that our work, <a href="https://www.ijcai.org/proceedings/2024/0193.pdf">ChatSpot</a>, has been accepted for a Long Oral presentation at IJCAI 2024! See you in Jeju!</li>

  <li><em>2023.12</em>: ¬†üéâüéâ We present <a href="https://ahnsun.github.io/merlin/">Merlin</a>, the first end-to-end multimodal large language model that supports video-level visual localization (including tracking, video recognition, video registration, etc.) and future reasoning.</li>
  <li>
    <p><em>2023.07</em>: ¬†üéâüéâ We present <a href="https://chatspot.streamlit.app/">ChatSpot</a>, a unified end-to-end multimodal large language model that supports diverse forms of interactivity including mouse clicks, drag-and-drop, and drawing boxes, which provides a more flexible and seamless interactive experience.</p>
  </li>
  <li><em>2023.05</em>: ¬†üéâüéâ We present <a href="https://chatspot.streamlit.app/">MOTRv3</a>, a fully end-to-end multiple object tracking model that achieves SOTA performance on DanceTrack, which outperforms the tracking-by-detection trackers for the first time.</li>
</ul>

<h1 id="-publications">üìù Highlight Publications</h1>

<div class="paper-box"><div class="paper-box-image"><div><div class="badge">NeuraIPS2025 Submission</div><img src="images/pr1.png" alt="sym" width="100%" /></div></div>
<div class="paper-box-text">

    <p><a href="https://arxiv.org/pdf/2504.07954">Perception-R1: Pioneering Perception Policy with Reinforcement Learning</a></p>

    <p><strong>En Yu</strong>, Kangheng Lin, Liang Zhao, Jisheng Yin, Yana Wei, Yuang Peng, Haoran Wei, Jianjian Sun, Chunrui Han, Zheng Ge, Xiangyu Zhang, Daxin Jiang, Jingyu Wang, Wenbing Tao </p>

    <p><a href="https://github.com/linkangheng/PR1"><strong>Project</strong></a></p>
    <ul>
      <li>Perception-R1 pioneers the exploration of RL's potential in MLLM post-training for perception policy learning. We get valuable cognition through experiments. It sets new SoTAs in visual perception tasks, especially object detection. Its novel paradigm enables it to match and surpass expert models, showing the great potential of perception policy learning. </li>
    </ul>
  </div>
</div>


<div class="paper-box"><div class="paper-box-image"><div><div class="badge">ICLR2025 Poster</div><img src="images/utr.png" alt="sym" width="100%" /></div></div>
<div class="paper-box-text">

    <p><a href="https://arxiv.org/pdf/2312.00589.pdf">Unhackable Temporal Rewarding for Scalable Video MLLMs</a></p>

    <p><strong>En Yu</strong>, Kangheng Lin, Liang Zhao, Yana Wei, Zining Zhu, Haoran Wei, Jianjian Sun, Zheng Ge, Xiangyu Zhang, Jingyu Wang, Wenbing Tao </p>

    <p><a href="https://ahnsun.github.io/UTR/"><strong>Project</strong></a></p>
    <ul>
      <li>This work investigates the shortcut learning in video multimodal large language models and systemally establish temporal hacking theory including: 
        (1) Systematic exploration of the video MLLM unscaling phenomenon, establishing temporal hacking theory from a novel RL perspective. 
        (2) Design of Temporal Perplexity (TPL) score, providing a reliable reference metric for mitigating temporal hacking. 
        (3) Proposing two principles to guide the design of proxy rewards for video-language modeling and further propose Unhackable Temporal Rewarding (UTR). </li>
    </ul>
  </div>
</div>

<div class="paper-box"><div class="paper-box-image"><div><div class="badge">ICLR2025 Poster</div><img src="images/ovtr.png" alt="sym" width="100%" /></div></div>
<div class="paper-box-text">

    <p><a href="https://arxiv.org/pdf/2312.00589.pdf">OVTR: End-to-End Open-Vocabulary Multiple Object Tracking with Transformer</a></p>

    <p>Jinyang Li, <strong>En Yu</strong>, Sijia Chen, Wenbing Tao </p>

    <p><a href="https://github.com/jinyanglii/OVTR"><strong>Project</strong></a></p>
    <ul>
      <li>OVTR serves as the first fully end-to-end open-vocabulary multiple-object tracking framework.</li>
    </ul>
  </div>
</div>

<div class="paper-box"><div class="paper-box-image"><div><div class="badge">ECCV2024 Poster</div><img src="images/merlin.png" alt="sym" width="100%" /></div></div>
<div class="paper-box-text">

    <p><a href="https://arxiv.org/pdf/2312.00589.pdf">Merlin: Empowering Multimodal LLMs with Foresight Minds</a></p>

    <p><strong>En Yu</strong>, Liang Zhao, Yana Wei, Jinrong Yang, Dongming Wu, Lingyu Kong, Haoran Wei, Tiancai Wang, Zheng Ge, Xiangyu Zhang, Wenbing Tao</p>

    <p><a href="https://ahnsun.github.io/merlin/"><strong>Project</strong></a></p>
    <ul>
      <li>Merlin is a groundbreaking model capable of generating natural language responses that are intricately linked with object trajectories. Merlin excels in predicting and reasoning about future events based on initial observations, showcasing an unprecedented capability in future prediction and reasoning.</li>
    </ul>
  </div>
</div>

<div class="paper-box"><div class="paper-box-image"><div><div class="badge">IJCAI2024 Long Oral</div><img src="images/chatspot.png" alt="sym" width="100%" /></div></div>
<div class="paper-box-text">

    <p><a href="https://arxiv.org/pdf/2307.09474.pdf">ChatSpot: Bootstrapping Multimodal LLMs via Precise Referring Instruction Tuning</a></p>

    <p>Liang Zhao<sup>*</sup>, <strong>En Yu<sup>*</sup></strong>, Zheng Ge, Jinrong Yang, Haoran Wei, Hongyu Zhou, Jianjian Sun, Yuang Peng, Runpei Dong, Chunrui Han, Xiangyu Zhang</p>

    <p><a href="https://chatspot.streamlit.app/"><strong>Project</strong></a></p>
    <ul>
      <li>ChatSpot is a a unified end-toend multimodal large language model that supports diverse forms of interactivity including mouse clicks, drag-and-drop, and drawing boxes, which provides a more flexible and seamless interactive experience.</li>
    </ul>
  </div>
</div>

<div class="paper-box"><div class="paper-box-image"><div><div class="badge">Arxiv Preprint</div><img src="images/motrv3.png" alt="sym" width="100%" /></div></div>
<div class="paper-box-text">

    <p><a href="https://arxiv.org/pdf/2305.14298.pdf">MOTRv3: Release-Fetch Supervision for End-to-End Multi-Object Tracking</a></p>

    <p><strong>En Yu</strong>, Tiancai Wang, Zhuoling Li, Yuang Zhang, Xiangyu Zhang, Wenbing Tao</p>

    <p><a href=""><strong>Project</strong></a></p>
    <ul>
      <li>MOTRv3 is a fully end-to-end multiple object tracking (MOT) model that outperforms existing SOTA tracking-by-detection methods without any assistance of an extra detection network or post-processing.</li>
    </ul>
  </div>
</div>

<div class="paper-box"><div class="paper-box-image"><div><div class="badge">RA-L</div><img src="images/grouplane.png" alt="sym" width="100%" /></div></div>
<div class="paper-box-text">

    <p><a href="https://arxiv.org/pdf/2307.09472.pdf">GroupLane: End-to-End 3D Lane Detection with Channel-wise Grouping</a></p>

    <p>Zhuoling Li, Chunrui Han, Zheng Ge, Jinrong Yang, <strong>En Yu</strong>, Haoqian Wang, Hengshuang Zhao, Xiangyu Zhang</p>

    <p><a href=""><strong>Project</strong></a></p>
    <ul>
      <li>GroupLane is the first fully-convoluition end-to-end 3D lane detection network. GroupLane achieves SOTA performance on existing mainstream lane detection benchmark, i.e., OpenLane, Once-3DLanes, and OpenLane-Huawei while also ensuring fast inference speed (7 x faster than PersFormer).</li>
    </ul>
  </div>
</div>

<div class="paper-box"><div class="paper-box-image"><div><div class="badge">IROS2024 Poster</div><img src="images/qtrack.png" alt="sym" width="100%" /></div></div>
<div class="paper-box-text">

    <p><a href="https://arxiv.org/pdf/2208.10976.pdf">Quality Matters: Embracing Quality Clues for Robust 3D Multi-Object Tracking</a></p>

    <p>Jinrong Yang<sup>*</sup>, <strong>En Yu<sup>*</sup></strong>, Zeming Li, Xiaoping Li, Wenbing Tao</p>

    <p><a href=""><strong>Project</strong></a></p>
    <ul>
      <li>QTrack achieves 51.1%, 54.8% and 56.6% AMOTA tracking performance on the nuScenes test sets with BEVDepth, VideoBEV, and StreamPETR models, respectively, which significantly reduces the performance gap between pure camera and LiDAR-based trackers.</li>
    </ul>
  </div>
</div>

<div class="paper-box"><div class="paper-box-image"><div><div class="badge">AAAI2023 Poster</div><img src="images/ltrack.png" alt="sym" width="100%" /></div></div>
<div class="paper-box-text">

    <p><a href="https://ojs.aaai.org/index.php/AAAI/article/view/25437">Generalizing multiple object tracking to unseen domains by introducing natural language representation</a></p>

    <p><strong>En Yu</strong>, Songtao Liu, Zhuoling Li, Jinrong Yang, Zeming Li, Shoudong Han, Wenbing Tao</p>

    <p><a href=""><strong>Project</strong></a></p>
    <ul>
      <li>We introudce LTrack, the first multiple-object tracking model supporting vision-language modality inputs. Thanks to the dimain invariant of natural language representation, LTrack achieves SOTA performance on our established cross-domain MOT benchmark.</li>
    </ul>
  </div>
</div>

<div class="paper-box"><div class="paper-box-image"><div><div class="badge">CVPR2022 Poster</div><img src="images/mtrack.png" alt="sym" width="100%" /></div></div>
<div class="paper-box-text">

    <p><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Yu_Towards_Discriminative_Representation_Multi-View_Trajectory_Contrastive_Learning_for_Online_Multi-Object_CVPR_2022_paper.pdf">Towards Discriminative Representation: Multi-view Trajectory Contrastive Learning for Online Multi-object Tracking</a></p>

    <p><strong>En Yu</strong>, Zhuoling Li, Shoudong Han</p>

    <p><a href=""><strong>Project</strong></a></p>
    <ul>
      <li>We propose MTrack that adopts multi-view trajectory contrastive learning, in which each trajectory is represented as a center vector. By maintaining all the vectors in a dynamically updated memory bank, a trajectory-level contrastive loss is devised to explore the inter-frame information in the whole trajectories. MTrack surpassed preceding trackers and established new SOTA performance.</li>
    </ul>
  </div>
</div>

<div class="paper-box"><div class="paper-box-image"><div><div class="badge">TMM</div><img src="images/relationtrack.png" alt="sym" width="100%" /></div></div>
<div class="paper-box-text">

    <p><a href="https://arxiv.org/pdf/2105.04322.pdf">Relationtrack: Relation-aware multiple object tracking with decoupled representation</a></p>

    <p><strong>En Yu</strong>, Zhuoling Li, Shoudong Han, Hongwei Wang</p>

  </div>
</div>

<ul>
  <li>
    <p><a href="https://arxiv.org/pdf/2009.04794.pdf">MAT: Motion-aware Multi-Object Tracking</a>, Shoudong Han, Piao Huang, Hongwei Wang, <strong>En Yu</strong>, Donghaisheng Liu, Xiaofeng Pan, <strong>Neurocomputing</strong></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/pdf/2209.00522.pdf">Implicit and Efficient Point Cloud Completion for 3D Single Object Tracking</a>, Pan Wang, Liangliang Ren, Shengkai Wu, Jinrong Yang, <strong>En Yu</strong>, Hangcheng Yu, Xiaoping Li, <strong>IEEE Robotics and Automation Letters</strong></p>
  </li>
  <li>
    <p><a href="https://ieeexplore.ieee.org/abstract/document/9763033">Efficient few-shot classification via contrastive pre-training on web data</a>, Zhuoling Li, Haohan Wang, Tymosteusz Swistek, <strong>En Yu</strong>, Haoqian Wang, <strong>IEEE Transactions on Artificial Intelligence</strong></p>
  </li>
</ul>

<h1 id="-honors-and-awards">üéñ Honors and Awards</h1>
<ul>
  <li><em>2022.05</em> Second Prize in the First Global Artificial Intelligence Technology Innovation Competition.</li>
  <li><em>2019.08</em> First Prize in the 13th National College Students‚Äô Intelligent Car Competition.</li>
  <li><em>2018.08</em> National Champion in the 14th National College Students‚Äô Intelligent Car Competition.</li>
</ul>

<h1 id="-educations">üìñ Educations</h1>
<ul>
  <li><em>2024.07 - 2025.05 (now)</em>, University of California, Santa Barbara (UCSB), USA.</li>
  <li><em>2022.06 - 2025.05 (now)</em>, Huazhong University of Science and Technology, China.</li>
  <li><em>2020.09 - 2022.06</em>, Huazhong University of Science and Technology, Whhan, China.</li>
  <li><em>2016.09 - 2020.06</em>, Huazhong University of Science and Technology, Wuhan, China.</li>
</ul>

<h1 id="-internships">üíª Internships</h1>
<ul>
  <li><em>2022.03 - 2024.03</em>, <a href="https://en.megvii.com/">MEGVII Research</a>, Foundation Model Group.</li>
  <li><em>2024.03 - 2025.05 (now)</em>, <a href="https://en.megvii.com/">StepFun AI</a>, Multimodal Intelligence Group.</li>
</ul>

          </section>
        </div>
      </article>
    </div>

    <script src="assets/js/main.min.js"></script>



<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id="></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', "");
</script>


<script>
    $(document).ready(function () {
        
        var gsDataBaseUrl = 'https://cdn.jsdelivr.net/gh/Ahnsun.github.io@'
        
        $.getJSON(gsDataBaseUrl + "google-scholar-stats/gs_data.json", function (data) {
            var totalCitation = data['citedby']
            document.getElementById('total_cit').innerHTML = totalCitation;
            var citationEles = document.getElementsByClassName('show_paper_citations')
            Array.prototype.forEach.call(citationEles, element => {
                var paperId = element.getAttribute('data')
                var numCitations = data['publications'][paperId]['num_citations']
                element.innerHTML = '| Citations: ' + numCitations;
            });
        });
    })
</script>


  </body>
</html>
